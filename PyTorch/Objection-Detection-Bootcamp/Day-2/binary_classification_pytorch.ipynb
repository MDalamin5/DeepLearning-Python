{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2082cc-d7e2-43b6-8eaf-837443bd7767",
   "metadata": {},
   "source": [
    "# Binary Classification with pytorch\n",
    "\n",
    "DATASET : https://www.kaggle.com/datasets/erdemtaha/cancer-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f68f4832-b717-4c68-b1fe-96751c7daab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3501dac-5c82-4597-aa0d-89c38815a20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
      "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
      "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
      "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
      "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
      "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
      "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
      "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
      "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
      "      dtype='object')\n",
      "====================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 33 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      " 32  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), int64(1), object(1)\n",
      "memory usage: 146.8+ KB\n",
      "None\n",
      "====================\n",
      "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
      "0    842302         M        17.99         10.38          122.80     1001.0   \n",
      "1    842517         M        20.57         17.77          132.90     1326.0   \n",
      "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
      "3  84348301         M        11.42         20.38           77.58      386.1   \n",
      "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
      "0  ...          17.33           184.60      2019.0            0.1622   \n",
      "1  ...          23.41           158.80      1956.0            0.1238   \n",
      "2  ...          25.53           152.50      1709.0            0.1444   \n",
      "3  ...          26.50            98.87       567.7            0.2098   \n",
      "4  ...          16.67           152.20      1575.0            0.1374   \n",
      "\n",
      "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
      "0             0.6656           0.7119                0.2654          0.4601   \n",
      "1             0.1866           0.2416                0.1860          0.2750   \n",
      "2             0.4245           0.4504                0.2430          0.3613   \n",
      "3             0.8663           0.6869                0.2575          0.6638   \n",
      "4             0.2050           0.4000                0.1625          0.2364   \n",
      "\n",
      "   fractal_dimension_worst  Unnamed: 32  \n",
      "0                  0.11890          NaN  \n",
      "1                  0.08902          NaN  \n",
      "2                  0.08758          NaN  \n",
      "3                  0.17300          NaN  \n",
      "4                  0.07678          NaN  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "cancer_df = pd.read_csv('./data/Cancer_Data.csv')\n",
    "\n",
    "print(cancer_df.columns)\n",
    "\n",
    "print(\"=\"*20)\n",
    "\n",
    "print(cancer_df.info())\n",
    "\n",
    "print(\"=\"*20)\n",
    "\n",
    "print(cancer_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7a2f24e-4af2-486f-8a6e-4b41d6a27e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnosis\n",
      "B    357\n",
      "M    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "diagnosis_counts = cancer_df['diagnosis'].value_counts()\n",
    "\n",
    "print(diagnosis_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16006c92-eb53-4a3f-9417-d2b9e56acf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "cancer_df = cancer_df.drop(columns=['id', 'Unnamed: 32'])\n",
    "\n",
    "# Convert diagnosis to binary: Malignant (M) = 1, Benign (B) = 0\n",
    "cancer_df['diagnosis'] = cancer_df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "\n",
    "# Split features and target\n",
    "X = cancer_df.drop(columns=['diagnosis'])\n",
    "y = cancer_df['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a0e89-a277-4733-a406-1c87f06d490a",
   "metadata": {},
   "source": [
    "# Scaling Vs Normalization\n",
    "\n",
    "\n",
    "#### Scaling\n",
    "Scaling refers to resizing the range of features to ensure they are on a similar scale.\n",
    "\n",
    "ex : Min-Max Scaling, Range 0 - 1\n",
    "\n",
    "X_scaled = (X - Xmin)/(X_max - X_min)\n",
    "\n",
    "We should use scaling when distances between points matter (e.g., KNN, gradient descent-based algorithms like neural networks).\n",
    "\n",
    "#### Normalization\n",
    "\n",
    "Normally, different features have different measurments which makes comparison difficult. If we dont standardize then one could dominate the calculations in machine learning models so we need a common scale.\n",
    "\n",
    "\n",
    "\n",
    "Normalization refers to rescaling the data so that it has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "We apply this for each column seperately and is also called Z-score normalization or standardization and both are statistical technique.\n",
    "\n",
    "Mean 0: \n",
    "- The mean (or average) is a measure of the central tendency of your data. It tells you where the \"center\" of your data is.\n",
    "\n",
    "- Now, if the mean is 0, it simply means that the center of the data has been shifted to zero. After standardization, the values will be centered around 0, which makes comparisons easier between features that were originally on different scales.\n",
    "\n",
    "Standard Deviation 1:\n",
    "\n",
    "- The standard deviation tells you how \"spread out\" or \"spread around\" the values are from the mean.\n",
    "\n",
    "- A high standard deviation means the data points are widely spread out, and a low standard deviation means they are closely packed around the mean.\n",
    "\n",
    "- Scaling needs to be done on all features so that the spread or variability of the data is standardized to be 1 unit so most of the data is within 1 standard deviation from the mean.\n",
    "\n",
    "- If the standard deviation is 1, it means that most of your data points lie within one unit (1) away from the mean in both directions. +1 and -1.\n",
    "\n",
    "- Data points  much higher or lower than the average will now be expressed in multiples of standard deviation\n",
    "\n",
    "- In short Standard deviation will give us the unit of 1 SD\n",
    "\n",
    "\n",
    "A = 100\n",
    "B = 200\n",
    "C = 300\n",
    "\n",
    "Mean = 200\n",
    "\n",
    "SD = 100 = 1 unit\n",
    "\n",
    "A = -1\n",
    "B = 0 \n",
    "C = +1\n",
    "\n",
    "Mu = mean\n",
    "Sigma = Standard Deviation\n",
    "\n",
    "X_norm = (X - Mu)/(Sigma)\n",
    "\n",
    "We should use Normalization when you need to center the data for algorithms sensitive to feature distribution and in algorithms that assume normally distributed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8812b00f-02a3-4ec4-b117-f5778ea91346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3845efe9-89a1-4e7c-bca7-a43849c23284",
   "metadata": {},
   "source": [
    "# Pytorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3564db10-83f8-430e-bc50-7035b4c62bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "908a57bd-7db8-42e0-ae39-bf75cd30d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Creating Dataset and Dataloader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12f68a-fea3-43ed-b2e1-3c3077ec2f4e",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6da16c3-e014-41e9-b034-47bcd0a3a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the Neural Network\n",
    "class CancerNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CancerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(30, 60)  # 30 input features\n",
    "        self.fc2 = nn.Linear(60, 80)\n",
    "        self.fc3 = nn.Linear(80, 100)\n",
    "        self.fc4 = nn.Linear(100, 140)\n",
    "        self.fc5 = nn.Linear(140, 180)\n",
    "        self.fc6 = nn.Linear(180, 150)\n",
    "        self.fc7 = nn.Linear(150, 100)\n",
    "        self.fc8 = nn.Linear(100, 50)\n",
    "        self.fc9 = nn.Linear(50, 25)\n",
    "        self.fc10 = nn.Linear(25, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)  # 30% dropout\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc8(x))\n",
    "        x = torch.relu(self.fc9(x))\n",
    "        x = self.sigmoid(self.fc10(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc6a115c-3683-4457-a579-d3366d854006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = CancerNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f05f4446-eab2-4569-8ca9-8b674b9edcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            labels = labels.unsqueeze(1)  # Add extra dimension to match the output shape\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69cd8fe4-e8c7-46b0-9b68-9e9f7590982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.48881040713749824\n",
      "Epoch 2/40, Loss: 0.1706023317296058\n",
      "Epoch 3/40, Loss: 0.0923114600746582\n",
      "Epoch 4/40, Loss: 0.04561207322403789\n",
      "Epoch 5/40, Loss: 0.03795632511319127\n",
      "Epoch 6/40, Loss: 0.024321097430220106\n",
      "Epoch 7/40, Loss: 0.38385028412255146\n",
      "Epoch 8/40, Loss: 0.36862138103072845\n",
      "Epoch 9/40, Loss: 0.34100768603384496\n",
      "Epoch 10/40, Loss: 0.06314152517673695\n",
      "Epoch 11/40, Loss: 0.19026650758387403\n",
      "Epoch 12/40, Loss: 0.0764404396196672\n",
      "Epoch 13/40, Loss: 0.06322785338718576\n",
      "Epoch 14/40, Loss: 0.03032026014601191\n",
      "Epoch 15/40, Loss: 0.47797748457320116\n",
      "Epoch 16/40, Loss: 0.15761693932581694\n",
      "Epoch 17/40, Loss: 0.049267973036815724\n",
      "Epoch 18/40, Loss: 0.036118607657651104\n",
      "Epoch 19/40, Loss: 0.022170197191429725\n",
      "Epoch 20/40, Loss: 0.026028451687943745\n",
      "Epoch 21/40, Loss: 0.01645579804390793\n",
      "Epoch 22/40, Loss: 0.008246669168744421\n",
      "Epoch 23/40, Loss: 0.053173351456555926\n",
      "Epoch 24/40, Loss: 0.5399745826919874\n",
      "Epoch 25/40, Loss: 0.02825996347981648\n",
      "Epoch 26/40, Loss: 0.532135641489225\n",
      "Epoch 27/40, Loss: 1.4960623087288978\n",
      "Epoch 28/40, Loss: 2.1659287210576292\n",
      "Epoch 29/40, Loss: 1.395467469261223\n",
      "Epoch 30/40, Loss: 1.096942237595734\n",
      "Epoch 31/40, Loss: 0.349187865198716\n",
      "Epoch 32/40, Loss: 0.9018690581123034\n",
      "Epoch 33/40, Loss: 0.25906900490323703\n",
      "Epoch 34/40, Loss: 1.6742143834009766\n",
      "Epoch 35/40, Loss: 1.2809868329569385\n",
      "Epoch 36/40, Loss: 0.13035720512270926\n",
      "Epoch 37/40, Loss: 0.621918025740888\n",
      "Epoch 38/40, Loss: 0.75148760620541\n",
      "Epoch 39/40, Loss: 0.8378929604377134\n",
      "Epoch 40/40, Loss: 2.0314400794469596\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, criterion, optimizer, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee702e-d139-40e2-896d-587280dc44db",
   "metadata": {},
   "source": [
    "## Model Evaluation|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bd9d1e6-c790-4517-a63d-898474d73ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            labels = labels.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()  # Sigmoid output to binary\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07cbb7b0-ff73-494b-8d10-75d64cdd6b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770518df-b4ad-42e1-b492-757858df6dbe",
   "metadata": {},
   "source": [
    "# Saving the prediction in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b372f83-c25d-4746-8912-66d72a731416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
